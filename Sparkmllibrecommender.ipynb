{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
    "rdd1.getStorageLevel()\n",
    "print(rdd1.getStorageLevel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mllib.recommendation âˆ’ Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user item association matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is performed using ALS algorithm to build the recommendation model and evaluate it on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement openjdk-8-jdk (from versions: none)\n",
      "ERROR: No matching distribution found for openjdk-8-jdk\n"
     ]
    }
   ],
   "source": [
    "!pip install openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 3.980378795013269e-06\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "   sc = SparkContext.getOrCreate()\n",
    "   data = sc.textFile(\"C:/Users/Dell/test.data\")\n",
    "   ratings = data.map(lambda l: l.split(','))\\\n",
    "      .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "   \n",
    "   # Build the recommendation model using Alternating Least Squares\n",
    "   rank = 10\n",
    "   numIterations = 10\n",
    "   model = ALS.train(ratings, rank, numIterations)\n",
    "   \n",
    "   # Evaluate the model on training data\n",
    "   testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "   predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "   ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "   MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "   print(\"Mean Squared Error = \" + str(MSE))\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
